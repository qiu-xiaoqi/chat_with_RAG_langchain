{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\LLM_Project\\RAG\\chat_with_RAG_langchain\\chain\n"
     ]
    }
   ],
   "source": [
    "from chat_QA_chain_self import Chat_QA_chain_self\n",
    "from QA_chain_self import QA_chain_self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: str = \"deepseek-chat\"\n",
    "temperature: float = 0.0\n",
    "top_k: int = 4\n",
    "chat_history: list = []\n",
    "file_path: str = r\"E:\\LLM_Project\\RAG\\chat_with_RAG_langchain\\knowledge_db\"\n",
    "persist_path: str = r\"E:\\LLM_Project\\RAG\\chat_with_RAG_langchain\\vector_db\"\n",
    "appid: str = None\n",
    "api_key: str = os.getenv(\"DeepSeek_API_for_RAG\")\n",
    "api_secret:str=None\n",
    "embedding = \"openai\"\n",
    "embedding_key = os.getenv(\"EMBEDDING_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = Chat_QA_chain_self(model=model,\n",
    "                              temperature=temperature,\n",
    "                              top_k=top_k,\n",
    "                              chat_history=chat_history,\n",
    "                              file_path=file_path,\n",
    "                              persist_path=persist_path,\n",
    "                              api_key=api_key,\n",
    "                              embedding=embedding,\n",
    "                              embedding_key=embedding_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chat_QA_chain_self.Chat_QA_chain_self at 0x27f38968ac0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer:[('我可以学习到关于强化学习的知识吗？', '是的，你可以学习到关于强化学习的知识。强化学习是机器学习的一个重要分支，主要研究智能体（agent）如何通过与环境互动来学习最优策略，以最大化累积奖励。以下是强化学习的核心内容：\\n\\n1. **基本概念**：\\n   - **智能体（Agent）**：学习的实体，通过行动与环境互动。\\n   - **环境（Environment）**：智能体所处的外部系统。\\n   - **状态（State）**：环境在某一时刻的描述。\\n   - **行动（Action）**：智能体在某一状态下采取的动作。\\n   - **奖励（Reward）**：智能体行动后环境给出的反馈信号。\\n   - **策略（Policy）**：智能体在给定状态下选择行动的规则。\\n\\n2. **关键算法**：\\n   - **Q-Learning**：一种无模型的强化学习算法，通过更新Q值来学习最优策略。\\n   - **Deep Q-Network (DQN)**：结合深度学习和Q-Learning，用于处理高维状态空间。\\n   - **Policy Gradient Methods**：直接优化策略，适用于连续动作空间。\\n   - **Actor-Critic Methods**：结合值函数和策略梯度的混合方法。\\n\\n3. **应用领域**：\\n   - 游戏（如AlphaGo、Atari游戏）。\\n   - 机器人控制。\\n   - 自动驾驶。\\n   - 资源管理（如计算资源分配）。\\n\\n4. **学习资源**：\\n   - 书籍：《Reinforcement Learning: An Introduction》（Richard S. Sutton and Andrew G. Barto）。\\n   - 在线课程：Coursera、Udacity上的强化学习专项课程。\\n   - 框架：OpenAI Gym、Stable Baselines、TensorFlow Agents。\\n\\n如果你有具体问题或想深入某个方面，可以进一步提问！')] chat_history:[('我可以学习到关于强化学习的知识吗？', '是的，你可以学习到关于强化学习的知识。强化学习是机器学习的一个重要分支，主要研究智能体（agent）如何通过与环境互动来学习最优策略，以最大化累积奖励。以下是强化学习的核心内容：\\n\\n1. **基本概念**：\\n   - **智能体（Agent）**：学习的实体，通过行动与环境互动。\\n   - **环境（Environment）**：智能体所处的外部系统。\\n   - **状态（State）**：环境在某一时刻的描述。\\n   - **行动（Action）**：智能体在某一状态下采取的动作。\\n   - **奖励（Reward）**：智能体行动后环境给出的反馈信号。\\n   - **策略（Policy）**：智能体在给定状态下选择行动的规则。\\n\\n2. **关键算法**：\\n   - **Q-Learning**：一种无模型的强化学习算法，通过更新Q值来学习最优策略。\\n   - **Deep Q-Network (DQN)**：结合深度学习和Q-Learning，用于处理高维状态空间。\\n   - **Policy Gradient Methods**：直接优化策略，适用于连续动作空间。\\n   - **Actor-Critic Methods**：结合值函数和策略梯度的混合方法。\\n\\n3. **应用领域**：\\n   - 游戏（如AlphaGo、Atari游戏）。\\n   - 机器人控制。\\n   - 自动驾驶。\\n   - 资源管理（如计算资源分配）。\\n\\n4. **学习资源**：\\n   - 书籍：《Reinforcement Learning: An Introduction》（Richard S. Sutton and Andrew G. Barto）。\\n   - 在线课程：Coursera、Udacity上的强化学习专项课程。\\n   - 框架：OpenAI Gym、Stable Baselines、TensorFlow Agents。\\n\\n如果你有具体问题或想深入某个方面，可以进一步提问！')]\n"
     ]
    }
   ],
   "source": [
    "question = \"我可以学习到关于强化学习的知识吗？\"\n",
    "answer = qa_chain.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------answer--------------:[('我可以学习到关于强化学习的知识吗？', '是的，你可以学习到关于强化学习的知识。强化学习是机器学习的一个重要分支，主要研究智能体（agent）如何通过与环境互动来学习最优策略，以最大化累积奖励。以下是强化学习的核心内容：\\n\\n1. **基本概念**：\\n   - **智能体（Agent）**：学习的实体，通过行动与环境互动。\\n   - **环境（Environment）**：智能体所处的外部系统。\\n   - **状态（State）**：环境在某一时刻的描述。\\n   - **行动（Action）**：智能体在某一状态下采取的动作。\\n   - **奖励（Reward）**：智能体行动后环境给出的反馈信号。\\n   - **策略（Policy）**：智能体在给定状态下选择行动的规则。\\n\\n2. **关键算法**：\\n   - **Q-Learning**：一种无模型的强化学习算法，通过更新Q值来学习最优策略。\\n   - **Deep Q-Network (DQN)**：结合深度学习和Q-Learning，用于处理高维状态空间。\\n   - **Policy Gradient Methods**：直接优化策略，适用于连续动作空间。\\n   - **Actor-Critic Methods**：结合值函数和策略梯度的混合方法。\\n\\n3. **应用领域**：\\n   - 游戏（如AlphaGo、Atari游戏）。\\n   - 机器人控制。\\n   - 自动驾驶。\\n   - 资源管理（如计算资源分配）。\\n\\n4. **学习资源**：\\n   - 书籍：《Reinforcement Learning: An Introduction》（Richard S. Sutton and Andrew G. Barto）。\\n   - 在线课程：Coursera、Udacity上的强化学习专项课程。\\n   - 框架：OpenAI Gym、Stable Baselines、TensorFlow Agents。\\n\\n如果你有具体问题或想深入某个方面，可以进一步提问！')]\n",
      "-----------------chat_history-----------------:[('我可以学习到关于强化学习的知识吗？', '是的，你可以学习到关于强化学习的知识。强化学习是机器学习的一个重要分支，主要研究智能体（agent）如何通过与环境互动来学习最优策略，以最大化累积奖励。以下是强化学习的核心内容：\\n\\n1. **基本概念**：\\n   - **智能体（Agent）**：学习的实体，通过行动与环境互动。\\n   - **环境（Environment）**：智能体所处的外部系统。\\n   - **状态（State）**：环境在某一时刻的描述。\\n   - **行动（Action）**：智能体在某一状态下采取的动作。\\n   - **奖励（Reward）**：智能体行动后环境给出的反馈信号。\\n   - **策略（Policy）**：智能体在给定状态下选择行动的规则。\\n\\n2. **关键算法**：\\n   - **Q-Learning**：一种无模型的强化学习算法，通过更新Q值来学习最优策略。\\n   - **Deep Q-Network (DQN)**：结合深度学习和Q-Learning，用于处理高维状态空间。\\n   - **Policy Gradient Methods**：直接优化策略，适用于连续动作空间。\\n   - **Actor-Critic Methods**：结合值函数和策略梯度的混合方法。\\n\\n3. **应用领域**：\\n   - 游戏（如AlphaGo、Atari游戏）。\\n   - 机器人控制。\\n   - 自动驾驶。\\n   - 资源管理（如计算资源分配）。\\n\\n4. **学习资源**：\\n   - 书籍：《Reinforcement Learning: An Introduction》（Richard S. Sutton and Andrew G. Barto）。\\n   - 在线课程：Coursera、Udacity上的强化学习专项课程。\\n   - 框架：OpenAI Gym、Stable Baselines、TensorFlow Agents。\\n\\n如果你有具体问题或想深入某个方面，可以进一步提问！')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"-----------------answer--------------:{answer}\")\n",
    "print(f\"-----------------chat_history-----------------:{qa_chain.chat_history}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"为什么这门课需要教这方面的知识？\"\n",
    "answer,chat_history = qa_chain.answer(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------answer--------------:('我可以学习到关于强化学习的知识吗？', '是的，你可以学习到关于强化学习的知识。强化学习是机器学习的一个重要分支，主要研究智能体（agent）如何通过与环境互动来学习最优策略，以最大化累积奖励。以下是强化学习的核心内容：\\n\\n1. **基本概念**：\\n   - **智能体（Agent）**：学习的实体，通过行动与环境互动。\\n   - **环境（Environment）**：智能体所处的外部系统。\\n   - **状态（State）**：环境在某一时刻的描述。\\n   - **行动（Action）**：智能体在某一状态下采取的动作。\\n   - **奖励（Reward）**：智能体行动后环境给出的反馈信号。\\n   - **策略（Policy）**：智能体在给定状态下选择行动的规则。\\n\\n2. **关键算法**：\\n   - **Q-Learning**：一种无模型的强化学习算法，通过更新Q值来学习最优策略。\\n   - **Deep Q-Network (DQN)**：结合深度学习和Q-Learning，用于处理高维状态空间。\\n   - **Policy Gradient Methods**：直接优化策略，适用于连续动作空间。\\n   - **Actor-Critic Methods**：结合值函数和策略梯度的混合方法。\\n\\n3. **应用领域**：\\n   - 游戏（如AlphaGo、Atari游戏）。\\n   - 机器人控制。\\n   - 自动驾驶。\\n   - 资源管理（如计算资源分配）。\\n\\n4. **学习资源**：\\n   - 书籍：《Reinforcement Learning: An Introduction》（Richard S. Sutton and Andrew G. Barto）。\\n   - 在线课程：Coursera、Udacity上的强化学习专项课程。\\n   - 框架：OpenAI Gym、Stable Baselines、TensorFlow Agents。\\n\\n如果你有具体问题或想深入某个方面，可以进一步提问！')\n",
      "-----------------chat_history-----------------:[('我可以学习到关于强化学习的知识吗？', '是的，你可以学习到关于强化学习的知识。强化学习是机器学习的一个重要分支，主要研究智能体（agent）如何通过与环境互动来学习最优策略，以最大化累积奖励。以下是强化学习的核心内容：\\n\\n1. **基本概念**：\\n   - **智能体（Agent）**：学习的实体，通过行动与环境互动。\\n   - **环境（Environment）**：智能体所处的外部系统。\\n   - **状态（State）**：环境在某一时刻的描述。\\n   - **行动（Action）**：智能体在某一状态下采取的动作。\\n   - **奖励（Reward）**：智能体行动后环境给出的反馈信号。\\n   - **策略（Policy）**：智能体在给定状态下选择行动的规则。\\n\\n2. **关键算法**：\\n   - **Q-Learning**：一种无模型的强化学习算法，通过更新Q值来学习最优策略。\\n   - **Deep Q-Network (DQN)**：结合深度学习和Q-Learning，用于处理高维状态空间。\\n   - **Policy Gradient Methods**：直接优化策略，适用于连续动作空间。\\n   - **Actor-Critic Methods**：结合值函数和策略梯度的混合方法。\\n\\n3. **应用领域**：\\n   - 游戏（如AlphaGo、Atari游戏）。\\n   - 机器人控制。\\n   - 自动驾驶。\\n   - 资源管理（如计算资源分配）。\\n\\n4. **学习资源**：\\n   - 书籍：《Reinforcement Learning: An Introduction》（Richard S. Sutton and Andrew G. Barto）。\\n   - 在线课程：Coursera、Udacity上的强化学习专项课程。\\n   - 框架：OpenAI Gym、Stable Baselines、TensorFlow Agents。\\n\\n如果你有具体问题或想深入某个方面，可以进一步提问！'), ('为什么这门课需要教这方面的知识？', '学习强化学习（Reinforcement Learning, RL）主要有以下几个重要原因，这些原因反映了它在理论和应用中的独特价值：\\n\\n---\\n\\n### 1. **解决复杂决策问题**\\n   - 强化学习专注于**序列决策问题**，即通过与环境交互学习最优策略。这类问题在传统方法（如规则编程或监督学习）中难以解决，例如：\\n     - 机器人控制（如行走、抓取）\\n     - 游戏AI（如AlphaGo、Dota 2的OpenAI Five）\\n     - 自动驾驶（动态路径规划）\\n\\n---\\n\\n### 2. **适应动态与不确定环境**\\n   - RL能处理环境的不确定性和实时变化，通过试错学习动态调整策略。例如：\\n     - 金融交易（市场波动下的投资策略）\\n     - 能源优化（电网负载动态分配）\\n     - 推荐系统（用户兴趣随时间变化）\\n\\n---\\n\\n### 3. **超越人类水平的性能**\\n   - 在特定领域（如游戏、棋类），RL算法已展现出超越人类专家的能力，例如：\\n     - AlphaZero（从零开始自学围棋、国际象棋）\\n     - DeepMind的Atari游戏AI（仅凭像素输入学习通关）\\n\\n---\\n\\n### 4. **无监督或弱监督学习**\\n   - RL不需要大量标注数据（与监督学习不同），而是通过**奖励信号**自主学习。这在数据标注成本高的场景（如医疗决策）中尤为重要。\\n\\n---\\n\\n### 5. **跨学科应用潜力**\\n   - RL与神经科学、心理学、经济学等交叉，例如：\\n     - 模仿人类/动物学习机制（多巴胺与奖励机制）\\n     - 经济学中的博弈论与多智能体交互\\n\\n---\\n\\n### 6. **技术发展的前沿方向**\\n   - RL是人工智能的核心领域之一，结合深度学习（如深度强化学习）后，持续推动AI边界。掌握RL有助于参与：\\n     - 通用人工智能（AGI）的探索\\n     - 新兴领域（如元学习、多智能体系统）\\n\\n---\\n\\n### 何时不需要RL？\\n   - 如果问题有大量静态标注数据，监督学习可能更高效。\\n   - 如果环境完全已知且可建模，传统优化方法（如动态规划）可能足够。\\n\\n---\\n\\n### 总结\\n强化学习的核心价值在于解决**动态、交互式、长期回报**的决策问题。随着AI在现实世界中的渗透，RL将成为自动化系统、自适应工具和智能体设计的核心技术之一。学习它不仅是为了掌握一种算法，更是为了培养解决复杂问题的思维方式。')]\n"
     ]
    }
   ],
   "source": [
    "print(f\"-----------------answer--------------:{answer}\")\n",
    "print(f\"-----------------chat_history-----------------:{qa_chain.chat_history}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "chat_history = qa_chain.clear_history()\n",
    "print(chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
